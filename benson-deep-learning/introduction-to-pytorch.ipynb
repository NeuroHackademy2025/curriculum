{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is one of the preeminent machine-learning and optimization libraries currently available. It contains a number of powerful features that drastically simplify the task of fitting models and training neural networks. While we won't have time in this tutorial to examine more than a few of the core features, there are many additional tutorials available online. This tutorial will roughly follow a few of the introductory PyTorch tutorials available at [pytorch.org/tutorials](https://pytorch.org/tutorials/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll be covering three topics, each briefly:\n",
    "1. The basic structure of objects in PyTorch\n",
    "2. Non-linear optimization\n",
    "3. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Noah C. Benson &lt;[nben@uw.edu](mailto:nben@uw.edu)&gt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic PyTorch Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, PyTorch appears to be somewhat like NumPy in that it gives the user a set of classes and functions for interacting with a `Tensor` type that behaves much like NumPy's `ndarray` type. Both NumPy and PyTorch, for example, define functions like `log`, `sin`, and `mean` that work with their respective array type. However, the `Tensor` and `ndarray` objects aren't interchangeable. This is because PyTorch `Tensor`s are intended for use in optimization problems, and thus they track a wide variety of data about what computations they have been used in. These data are critical for performing efficient gradient-descent parameter-tuning, which is generally required for optimization and for training neural networks.\n",
    "\n",
    "Let's start by importing a number of PyTorch modules for use in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PyTorch library:\n",
    "import torch\n",
    "# Import the Neural-Network sub-module of PyTorch:\n",
    "from torch import nn\n",
    "# The DataLoader class is a helper for loading datasets during model training:\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Finally, we want to import matplotlib.pyplot so that we can visualize the\n",
    "# results of our training.\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of PyTorch revolves around a single datatype, the PyTorch `Tensor` (`torch.Tensor`). At first glance, the tensor class seems almost identical to NumPy's `ndarray` class. However, although they share many similarities, the two types are not directly compatible. The main differences between them are:\n",
    "1. tensors track a variety of data that can be used to calculate the gradient of whatever computation one uses them in, and\n",
    "2. tensors have a `device` parameter that allows one to specify `'cpu'` or `'cuda'` (when GPUs are available for calculations).\n",
    "\n",
    "This notebook uses the default `device` value, `'cpu'`, so all of our calculations will run on CPUs, which is no problem for the kinds of calculations in this notebook. GPUs can be much faster for certain kinds of computations, especially those involving matrix convolution, which we will discuss later. To use the `'cuda'` option, you need both to have GPUs available and to have the appropriate software and drivers installed.\n",
    "\n",
    "Let's take a closer look at the `torch.Tensor` type here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The torch library uses many of the same conventions as the numpy library.\n",
    "u = torch.zeros((10, 3))  # Make a 10x3 matrix.\n",
    "\n",
    "u[:,0] = 1 # Assign the first column to have values of 1.\n",
    "\n",
    "u = (u - 0.5) *  3.0 # Subtract 1/2 then multiply the tensor by 3.\n",
    "\n",
    "print(u) # Print the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we will sometimes get an error if we try to use numpy arrays and\n",
    "# tensors interchangeably. Although some operations like this may work, you\n",
    "# should generally work with one or the other and not try to mix them.\n",
    "import numpy as np\n",
    "\n",
    "a = np.zeros((10,3))\n",
    "\n",
    "torch.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, if you need to extract a numpy array from a tensor, you can\n",
    "# use first detach the tensor (i.e., remove it from the backend system\n",
    "# that keeps track of gradients) then request the numpy representation.\n",
    "a = u.detach().numpy()\n",
    "print(type(a))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to convert a numpy array into a tensor, you can also use the\n",
    "# torch.from_numpy() function.\n",
    "a = np.linspace(0, 1, 10)\n",
    "\n",
    "b = torch.from_numpy(a)\n",
    "print(type(b))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyTorch library includes analog functions for most of NumPy's basic API;\n",
    "for example, functions like `numpy.mean`, `numpy.std`, `numpy.exp`,\n",
    "`numpy.log`, `numpy.sum`, `numpy.median`, and `numpy.sin` have PyTorch\n",
    " analogs `torch.mean`, `torch.std`, `torch.exp`, `torch.log`, `torch.sum`,\n",
    "`torch.median`, and `torch.sin`. Most of these functions work similarly in\n",
    "both libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check that the interfaces are the same, let's make some identical\n",
    "# arrays/tensors:\n",
    "a = np.linspace(0, 1, 10)\n",
    "u = torch.linspace(0, 1, 10)\n",
    "\n",
    "print('Sum:')\n",
    "print(' -', np.sum(a))\n",
    "print(' -', torch.sum(u))\n",
    "\n",
    "print('Standard Deviation:')\n",
    "print(' -', np.std(a))\n",
    "print(' -', torch.std(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the standard deviations produced by the two libraries aren't\n",
    "identical!\n",
    "\n",
    "This difference occurs because the libraries use slightly different formulas\n",
    "for the standard deviation. (In brief: there are different formulas for the\n",
    "standard deviation depending on whether you are trying to characterize the\n",
    "set of values you have or trying to infer something about the\n",
    "population/distribution from which the values were drawn.)\n",
    "\n",
    "The point here is that it's important to verify that a PyTorch function does\n",
    "what you think it does. Most of them operate similarly to NumPy, but it can't\n",
    "be taken for granted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest kind of optimization problem involves some function `f(x)` that returns a single real value. Typically, the goal of the optimization is to determine the value(s) of `x` that produce the smallest value of `f(x)`. (Note that the `x` here might be a single number or a vector, but `f(x)` returns a single number.)\n",
    "\n",
    "During optimization, we typically don't know how `f(x)` is calculated, but we can calculate `f(x)` for any given `x`.  For example, in Python code, we might define an optimization function as follows:\n",
    "\n",
    "```python\n",
    "def minimize(f):\n",
    "    \"\"\"Given a function `f` as an argument, attempts to find the value `x`\n",
    "    such that `f(x)` is the minimum value of `f`. The return value is `x`.\n",
    "    \"\"\"\n",
    "    # In this optimization function, we can call f(x):\n",
    "    y0 = f(0)\n",
    "    y1 = f(1)\n",
    "    # But we don't know we don't know how f is calculating its outputs.\n",
    "    ...\n",
    "\n",
    "# This function could be minimized analytically, if we knew its form:\n",
    "def fn_to_minimize(x):\n",
    "    return (x - 2)**2 + (x - 1)**3\n",
    "\n",
    "# But there's no way for the minimize function to know the form:\n",
    "x_min = minimize(fn_to_minimize)\n",
    "```\n",
    "\n",
    "Most nonlinear optimization starts by making a guess as to what `x`-value yields the minimum `f(x)`, then calculating `f(x)` and using some strategy to make a new guess that is probably even closer to the minimum. If the value of `f(x)` for the new `x`-value is greater, then a different guess can be made; it's it's lesser, then the process can be repeated.\n",
    "\n",
    "The most common way, given `x1` and `f(x1)`, to make a guess of a value `x2` so that `f(x2) < f(x1)`, is to calculate the derivative of `df(x) / dx` in terms of `x` at `x1`. The derivative always points in the direction of fastest increase (so a positive derivative means the function is increasing and a negative derivative means it's decreasing), so by taking a small step away from `x1` in the opposite direction of the derivative, one can get a little closer to the minimum.\n",
    "\n",
    "This method works just as well with multiple parameters (or parameter vectors) as it does with a single parameter, as long as the function outputs a single number that can be minimized. In multidimensional functions, the gradient of the parameters points in the direction of fastest increase just like in single parameter functions.\n",
    "\n",
    "As an example of how an optimization problem is typically solved in PyTorch, we will start with a very simple optimization problem: find the minimum of `f(x) = (x - 2)**2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define the function that we're minimizing:\n",
    "def func(x):\n",
    "    return (x - 2)**2\n",
    "\n",
    "# If we create a PyTorch tensor that tracks its gradient, we can have PyTorch\n",
    "# automatically calculate the derivative of `func` for us.\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "# To do so, we call `func(x)`\n",
    "y = func(x)\n",
    "# Then call the backward gradient algorithm:\n",
    "y.backward()\n",
    "# Then the gradient is `x.grad`:\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize `func(x)`, we can use the gradients along with an optimizer\n",
    "object. PyTorch has several optimizer objects that use slightly different\n",
    "strategies, most of which are based on gradient descent. We'll use one called\n",
    "`SGD` which stands for Stochastic Gradient Descent. (The \"stochastic\" part of\n",
    "the name is due to the algorithm's ability to handle training when random\n",
    "subsets of a training dataset are used iteratively--the algorithm itself\n",
    "doesn't incorporate stochastic decisions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to start the minimization with x equal to this value:\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "# Declare an optimizer (SGD: stochastic gradient descent).\n",
    "# We are minimizing over the argument t (i.e., the input), and\n",
    "# we provide a low learning-rate (which affects how big the\n",
    "# optimizer's steps are).\n",
    "optimizer = torch.optim.SGD([x], lr=0.1)\n",
    "\n",
    "# Now we can take several steps to see if the optimizer converges.\n",
    "for step_number in range(40): # we'll take 20 steps...\n",
    "    # We're starting a new step, so we reset the gradients.\n",
    "    optimizer.zero_grad()\n",
    "    loss = func(x)\n",
    "    print(\"Step number\", step_number,\n",
    "          \"  x = \", float(x),\n",
    "          \"  loss = \", float(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(x, func(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combination of calling `output.backward()` then examining the `input.grad` value works not just for single values, but also for high-dimensional tensors, and PyTorch is very efficient about calculating these gradients.\n",
    "\n",
    "Because we can calculate the gradient for our loss function, we can minimize the loss by using a simple gradient descent optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above example, because the optimization is a simple gradient descent, the starting point can potentially change the result. Specifically, the optimization algorithm will typically find the nearest local minimum to the starting point. We used a function with only one minimum value, but if `func(x)` were equal to `sin(x)`, then the algorithm would find whatever minimum was nearest to the initial `x`-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network to recognize images of handwritten numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of this tutorial provides an example walkthrough of how to train a neural network using PyTorch. We will use a public datasetof written numerals, the [MNIST database](https://en.wikipedia.org/wiki/MNIST_database), and we will setup a simple neural network that can be trained to recognize the number represented in a small (28x28) image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we'll be using the MNIST dataset, which contains images of handwritten numerals (0-9), each of which has been labeled. The dataset is available on various locations around the internet, but it has been prepopulated on the JupyterHub for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data.\n",
    "(training_labels, training_images) = torch.load(\n",
    "    '/home/jovyan/shared/data/benson-deep-learning/mnist_train.pt',\n",
    "    weights_only=True)\n",
    "\n",
    "# Load test data.\n",
    "(test_labels, test_images) = torch.load(\n",
    "    '/home/jovyan/shared/data/benson-deep-learning/mnist_test.pt',\n",
    "    weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a representation of this dataset in the variables `training_labels`/`trainig_image` and `test_labels`/`test_images`. Naturally, these will correspond to our training and testing datasets; these are independent in order to avoid overfitting.\n",
    "\n",
    "To use these with PyTorch, we will need to convert them into PyTorch `Dataset` classes. Datasets simply contain a number of data-points (images of numerals in our case) and paired with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images.float()\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, ii):\n",
    "        return (self.images[ii], self.labels[ii])\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "training_data = MNISTDataset(training_images, training_labels)\n",
    "test_data = MNISTDataset(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch typically expects to interact with trainig and testing data via a class called `DataLoader`, which we imported earlier, from the `torch.utils.data` module. This class manages the loading and caching of individual data samples from our datasets, and can even perform loading in parallel if the dataset is large and cumbersome enough. In this cell, we setup data loaders for the two datasets and demonstrate how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data usually arrives to the model-fitting routine as \n",
    "# a batch of samples. This sets the size of each batch.\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# Extract a random image/label pair and print their metadata:\n",
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X [Batch-Size, Image-Channels, Height, Width]: \")\n",
    "    print(\"     \", X.shape, X.dtype)\n",
    "    print(\"Shape of y [1 Label per Batch]: \")\n",
    "    print(\"     \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we loop over the `test_dataloader` object, it yields a sequence of `(X,y)` tuples where `X` is a tensor of clothing images from the Fashion MNIST dataset and `y` is a list of the corresponding integer labels. Notice that the first dimension of both `X` and `y` is 64, which is also our training batch size--i.e., the dataloaders always yield data samples in batches. Notice also that the second dimension of the `X` value is 1; this is because the images are grayscale and thus have only one color-channel. The final two dimensions are the height and width of the image.\n",
    "\n",
    "The `y` labels that are returned are just integers, 0-9, corresponding to the associated numeral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if we can verify that these labels are correct by looking at a few of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll and label 4 images in a 2x2 grid:\n",
    "(fig, axs) = plt.subplots(2, 2, figsize=(4,4))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Get one batch of samples from the dataloader--remember that this\n",
    "# batch will have 64 images in it.\n",
    "for (X_batch,y_batch) in train_dataloader:\n",
    "    # Go ahead and plot the first four of these images\n",
    "    for (ax,X,y) in zip(axs, X_batch, y_batch):\n",
    "        ax.imshow(X)\n",
    "        ax.set_title(y.item())\n",
    "        ax.axis('off')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step for our neural network project is to define the neural network itself.\n",
    "\n",
    "PyTorch makes defining neural network models fairly easy. You simply need to declare a Python class that inherits from the `torch.nn.Module` class, which represents a single module of a neural network. Because a module can itself contain a number of network layers, the module can either represent a piece of a network or it can be the entire neural network.\n",
    "\n",
    "When declaring a `Module`, we need to make sure to define the stack of layers in the network in the class's constructor (the `__init__()` method), and we need to declare how the model is calculated in the class's `forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this network is a stack of a few operators. First of all, we have a layer that flattens the inputs from 28x28 images into 784-element vectors. Next, we have a sub-stack of layers that consists of three [linear operators](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html), each of which is rectified by a [rectified linear unit](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html).\n",
    "\n",
    "We aren't going to worry too much about what these particular layers of the network are doing. Suffice to say that these are fairly common components of neural networks, and that a full discussion of common neural network layers is beyond the scope of this tutorial.\n",
    "\n",
    "One other thing to note is that the `out_features` of the final linear operator in the stack is 10. This means that the output is in fact a 10-dimensional tensor (like a numpy array whose shape is `(10,)`). Typically when performing this kind of classification problem we interpret each of the output dimensions as representing the likelihood of an input to the model belonging to one of the dataset classes, so if the output features for a particular image are `[0, 0.1, 0, 0.01, 0.5, 0.3, 0.2, 0, 0,3]`, we interpret the model as predicting that the image belongs to class 4 (because `output[4]` is 0.5, which is the largest value in the outputs). In short, we can convert the output feature tensor into a predicted class number by taking the `argmax` of the output tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In order to train the above neural network, we will need to define a loss function that we are trying to minimize. In this case, we can use a builtin loss function called [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). We won't discuss the details of how this loss function works, but it is commonly used with classification problems like the one we are encoding here.\n",
    "\n",
    "We'll also need an optimizer, and in this case we'll use the same optimizer we used above: SGD (stochastic gradient descent).  Because our model was written using the `torch.nn.Module` class, we can get all of the model parameters by calling the `model.parameters()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the training itself, we just need to step through the data-items in the training dataset and provide them each to the optimizer, much like we did in the simpler optimization example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the training dataset.\n",
    "size = len(train_dataloader.dataset)\n",
    "\n",
    "# Walk through each training batch:\n",
    "for (batch_num, (X, y)) in enumerate(train_dataloader):\n",
    "    # Compute prediction:\n",
    "    pred = model(X)\n",
    "    # And compute the loss of that prediction:\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print out a status message every so often.\n",
    "    if batch_num % 100 == 0:\n",
    "        (loss, current) = (loss.item(), batch_num * len(X))\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it appears that the above cell worked, but what did it do? We can see from the printed lines that as the optimization proceeded, the loss decreased. If we want to see how well this fitting procedure worked, we can look at some examples from the test dataset and see how well the trained model performs. For this, we can essentially copy-and-paste the code-block above that we used to look at the initial images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll and label 4 images in a 2x2 grid:\n",
    "(fig, axs) = plt.subplots(2, 2, figsize=(6,6))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Get one batch of samples from the dataloader--remember that this\n",
    "# batch will have 64 images in it.\n",
    "for (X_batch,y_batch) in test_dataloader:\n",
    "    # We want a random set of 4 images each time, so we keep drawing\n",
    "    # new image batches until we randomly draw a number over 0.9:\n",
    "    if np.random.rand() < 0.9: continue\n",
    "    # Go ahead and plot the first four of these images\n",
    "    for (ax,X,y) in zip(axs, X_batch, y_batch):\n",
    "        ax.imshow(X)\n",
    "        # Get the model's prediction for this particular image:\n",
    "        pred = model(X[None,:])\n",
    "        # Convert the predicted tensor into a single label:\n",
    "        label = torch.argmax(pred)\n",
    "        # If that label is equal to y, the network got it right;\n",
    "        # otherwise, it got it wrong!\n",
    "        y_name = str(y.item())\n",
    "        label_name = str(label.item())\n",
    "        ax.set_title(f\"Estimate: {label_name}; True: {y_name}\")\n",
    "        ax.axis('off')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, our network isn't perfect--if you run the above cell many times, you will see that sometimes the network gets the item type correct, and sometimes it gets it wrong. However, you might also notice that when the network is wrong, it is wrong in a fairly understandable way (for example, 6 might be labeled a 0 or a 2 might be labeled a 7). This shouldn't be too surprising, considering that we trained a fairly small neural network with a simple architecture, but hopefully this example demonstrates the fundamentals of how PyTorch organizes models and networks used for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional neural network is a specific kind of neural network.\n",
    "Fundamentally, it operates on the same principles: the computation starts with a set of numbers that get filtered by a set of weights into a new layer of numbers; these numbers then get filtered by more layers, etc. The difference is primarily in how the networks are organized.\n",
    "\n",
    "CNNs typically operate on images, and their parameters (weights) are small images called kernels that are convoled with the inputs. Image convolutions are difficult to explain with words, but [this animation](https://en.m.wikipedia.org/wiki/File:2D_Convolution_Animation.gif) does a good job of explaining the basic idea. In essence, each image kernel represents a filter that gets applied to each position/pixel of the image and that pixel's immediate neighborhood in the image to produce a new value for the next layer of the network.\n",
    "\n",
    "The image convolutions that a model performs can vary according to several meta-parameters, primarily:\n",
    "* they can be larger or smaller, thus operating over a larger or smaller neighborhood of the input image;\n",
    "* they can have a `stride`, instructing them to skip over pixels in the input when producing the output thus decreasing the size of the output image;\n",
    "* they can pad their inputs to produce outputs of the same size (alternatively, the images decrease in size slightly due to pixel loss around the edges).\n",
    "\n",
    "We can build a CNN using the `nn.Conv2d` module object; aside from the use of this type, the model is very similar to the `NeuralNetwork` type we built earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv2d_relu_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 2, kernel_size=3, stride=1), #26x26\n",
    "            nn.Conv2d(2, 2, kernel_size=3, stride=1), #24x24\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2, 8, kernel_size=5, stride=2), #10x10\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1), #8x8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 10, kernel_size=7, stride=2),\n",
    "            nn.Flatten()) #1x1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.conv2d_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = ConvolutionalNeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the network in a very similar manner as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the training dataset.\n",
    "size = len(train_dataloader.dataset)\n",
    "\n",
    "# Make the optimizer:\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Walk through each training batch:\n",
    "for (batch_num, (X, y)) in enumerate(train_dataloader):\n",
    "    # Compute prediction:\n",
    "    pred = model(X[:,None,...])\n",
    "    # And compute the loss of that prediction:\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print out a status message every so often.\n",
    "    if batch_num % 100 == 0:\n",
    "        (loss, current) = (loss.item(), batch_num * len(X))\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can validate the CNN model as we did for the earlier neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll and label 4 images in a 2x2 grid:\n",
    "(fig, axs) = plt.subplots(2, 2, figsize=(6,6))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Get one batch of samples from the dataloader--remember that this\n",
    "# batch will have 64 images in it.\n",
    "for (X_batch,y_batch) in test_dataloader:\n",
    "    # We want a random set of 4 images each time, so we keep drawing\n",
    "    # new image batches until we randomly draw a number over 0.9:\n",
    "    if np.random.rand() < 0.9: continue\n",
    "    # Go ahead and plot the first four of these images\n",
    "    for (ax,X,y) in zip(axs, X_batch, y_batch):\n",
    "        ax.imshow(X)\n",
    "        # Get the model's prediction for this particular image:\n",
    "        pred = model(X[None,:])\n",
    "        # Convert the predicted tensor into a single label:\n",
    "        label = torch.argmax(pred)\n",
    "        # If that label is equal to y, the network got it right;\n",
    "        # otherwise, it got it wrong!\n",
    "        y_name = str(y.item())\n",
    "        label_name = str(label.item())\n",
    "        ax.set_title(f\"Estimate: {label_name}; True: {y_name}\")\n",
    "        ax.axis('off')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional PyTorch materials can be found primarily at [pytorch.org](https://pytorch.org/) (note specifically the [Docs](https://pytorch.org/docs/) and [Tutorials](https://pytorch.org/tutorials/) links at the top of the page). You may also want to check out PyTorch's [60-minute blitz video tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
